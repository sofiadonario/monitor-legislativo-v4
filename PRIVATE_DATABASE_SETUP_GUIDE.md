# Private Database Setup Guide
## Monitor Legislativo v4 - Complete Refactoring Implementation

üéØ **Goal**: Transform from external API calls to private database with periodic LexML collection

## ‚úÖ **Current Status - NEARLY COMPLETE!**

All major components have been implemented:
- ‚úÖ Database schema migration (`migrations/002_periodic_collection_schema.sql`)
- ‚úÖ Periodic collection service (`core/periodic_collection/lexml_collector.py`)
- ‚úÖ Private database API router (`main_app/routers/private_database_router.py`) 
- ‚úÖ Automation scheduler (`scripts/periodic_collection_scheduler.py`)
- ‚úÖ R-Shiny integration (`r-shiny-app/R/private_database_client.R`)
- ‚úÖ **JUST FIXED**: FastAPI router integration

## üîß **Final Setup Steps (Required)**

### Step 1: Configure Database URL

**For Supabase (your current setup):**

1. Get your Supabase database URL:
   - Go to your Supabase project dashboard
   - Navigate to Settings ‚Üí Database
   - Copy the "Connection string" (URI mode)

2. Create a `.env` file in your project root:
```bash
# Create .env file
touch .env
```

3. Add your DATABASE_URL to `.env`:
```env
# Supabase Database Configuration
DATABASE_URL=postgresql://[username]:[password]@[host]:[port]/[database]?sslmode=require

# Example format:
# DATABASE_URL=postgresql://postgres.abcdefghij:your_password@aws-0-us-east-1.pooler.supabase.com:5432/postgres?sslmode=require
```

### Step 2: Apply Database Migration

```bash
# Option A: Direct SQL execution (simplest)
# Copy the contents of migrations/002_periodic_collection_schema.sql
# and execute it in your Supabase SQL editor

# Option B: Using Python script
python3 -c "
import asyncio
import os
import asyncpg

async def apply_migration():
    database_url = os.getenv('DATABASE_URL')
    conn = await asyncpg.connect(database_url)
    
    with open('migrations/002_periodic_collection_schema.sql', 'r') as f:
        migration_sql = f.read()
    
    await conn.execute(migration_sql)
    await conn.close()
    print('‚úÖ Migration applied successfully!')

asyncio.run(apply_migration())
"
```

### Step 3: Test Private Database System

```bash
# Test database connectivity
python3 -c "
import os
import asyncio
import asyncpg

async def test_system():
    database_url = os.getenv('DATABASE_URL')
    conn = await asyncpg.connect(database_url)
    
    # Test migration applied
    search_terms = await conn.fetchval('SELECT COUNT(*) FROM search_terms_config')
    print(f'‚úÖ Found {search_terms} configured search terms')
    
    # Test private documents table
    docs = await conn.fetchval('SELECT 0')  # Test table exists
    print('‚úÖ Private documents table ready')
    
    await conn.close()
    print('‚úÖ Database system fully functional!')

asyncio.run(test_system())
"
```

### Step 4: Load CSV Fallback Data (Recommended)

Load the existing 889 transport legislation documents as your first batch:

```bash
# Load CSV fallback data into private database
python3 scripts/load_csv_fallback_data.py
```

This will:
- ‚úÖ Insert 889 real LexML transport documents into `private_legislative_documents`
- ‚úÖ Create search term configurations based on CSV data
- ‚úÖ Update state density statistics for geographic mapping
- ‚úÖ Generate collection execution logs
- ‚úÖ Provide immediate data for testing your private database

### Step 5: Run Initial Collection (Optional)

```bash
# Manual collection test for additional data
python3 scripts/periodic_collection_scheduler.py
```

### Step 6: Test Private Database Setup

```bash
# Comprehensive test of private database setup
python3 scripts/test_private_database.py
```

This will verify:
- ‚úÖ Database schema and tables exist
- ‚úÖ CSV data (889 documents) loaded correctly
- ‚úÖ Search terms configuration
- ‚úÖ State density data for mapping
- ‚úÖ Collection execution logs
- ‚úÖ Full-text search functionality

### Step 7: Test Private Database API

```bash
# Start your FastAPI server
uvicorn main_app.main:app --reload

# Test private database endpoints (in another terminal):
curl "http://localhost:8000/api/private/health"
curl "http://localhost:8000/api/private/analytics"
curl "http://localhost:8000/api/private/search?query=transporte"
curl "http://localhost:8000/api/private/state-density"
```

## üéØ **Architecture Transformation Achieved**

**Before:**
```
Dashboard ‚Üí External APIs (LexML, Regional) ‚Üí Real-time data
```

**After:**
```
1. Periodic Collector ‚Üí LexML API ‚Üí Private Database (monthly)
2. Dashboard ‚Üí Private Database ‚Üí Cached/Processed data  
3. Users ‚Üí Dashboard ‚Üí Private Database searches only
```

## üìä **Available Private Database Endpoints**

Once setup is complete, your dashboard will use these **private database** endpoints instead of external APIs:

- `GET /api/private/search` - Full-text search with PostgreSQL
- `GET /api/private/state-density` - Geographic document distribution
- `GET /api/private/analytics` - Database analytics and monitoring
- `GET /api/private/recent-collections` - Collection execution logs
- `POST /api/private/trigger-collection` - Manual collection trigger
- `GET /api/private/health` - Private database health check

## üîÑ **Automated Collection**

Set up cron job for automated monthly collection:

```bash
# Edit crontab
crontab -e

# Add monthly collection (1st day of month at 2 AM)
0 2 1 * * cd /path/to/project && python3 scripts/periodic_collection_scheduler.py
```

## üìà **R-Shiny Integration**

Your R-Shiny app is ready to connect directly to the private database:

```r
# The private_database_client.R is already configured
# Just ensure DATABASE_URL is available in R environment
source("R/private_database_client.R")
```

## üèÜ **Success Criteria**

After completing setup, you should have:

1. ‚úÖ **Private Database**: 8 transport legislation search terms configured
2. ‚úÖ **Monthly Collection**: Automated LexML data collection  
3. ‚úÖ **Dashboard Integration**: All searches use private database
4. ‚úÖ **Geographic Mapping**: State density visualization from private data
5. ‚úÖ **R-Shiny Analytics**: Direct database connectivity
6. ‚úÖ **Performance**: Cached responses, no external API dependencies
7. ‚úÖ **Monitoring**: Collection logs and database health checks

## üö® **Troubleshooting**

### Database Connection Issues
```bash
# Test basic connection
python3 -c "import asyncpg; print('AsyncPG available')"

# Check URL format
echo $DATABASE_URL
```

### Migration Issues
```bash
# Check if tables exist
python3 -c "
import asyncio, asyncpg, os
async def check():
    conn = await asyncpg.connect(os.getenv('DATABASE_URL'))
    tables = await conn.fetch(\"SELECT table_name FROM information_schema.tables WHERE table_schema='public'\")
    print('Tables:', [r[0] for r in tables])
asyncio.run(check())
"
```

### Collection Issues
```bash
# Check logs
tail -f logs/collection_scheduler_$(date +%Y%m%d).log
```

## üéâ **Congratulations!**

Once these final steps are complete, your Monitor Legislativo v4 will have successfully transformed from a real-time external API system to a **private database system with periodic collection**, exactly as planned in your comprehensive roadmap! 